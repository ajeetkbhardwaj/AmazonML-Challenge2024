{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1FY-pBtz1wc"
      },
      "source": [
        "# Data Loading and Extracting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glxMOzivz1wd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    # Open the image file\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Resize the image\n",
        "    img = img.resize(target_size)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Normalize the image to range [0, 1]\n",
        "    img_array = img_array / 255.0\n",
        "\n",
        "    return img_array\n",
        "\n",
        "# Example usage\n",
        "image_path = \"downloaded_images/image_0.jpg\"\n",
        "preprocessed_image = preprocess_image(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_entity_value(value):\n",
        "    # Remove any extra spaces\n",
        "    value = value.strip()\n",
        "\n",
        "    # Standardize units (example: replace 'gms' with 'gram')\n",
        "    value = re.sub(r'\\bgms\\b', 'gram', value)\n",
        "    value = re.sub(r'\\bmilligrams\\b', 'milligram', value)\n",
        "\n",
        "    # Ensure the number is in a consistent format\n",
        "    match = re.match(r'([\\d.]+)\\s*(\\w+)', value)\n",
        "    if match:\n",
        "        number, unit = match.groups()\n",
        "        # Convert number to a float and format it\n",
        "        number = float(number)\n",
        "        formatted_value = f\"{number} {unit}\"\n",
        "        return formatted_value\n",
        "    return value\n",
        "\n",
        "# Apply the cleaning function to the 'entity_value' column\n",
        "train_df['cleaned_entity_value'] = train_df['entity_value'].apply(clean_entity_value)\n",
        "\n",
        "# Example usage\n",
        "print(train_df[['entity_value', 'cleaned_entity_value']].head())\n"
      ],
      "metadata": {
        "id": "4CXYrFI22K-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the dataset\n",
        "missing_data = train_df.isnull().sum()\n",
        "\n",
        "# Optionally drop rows with missing values or handle them\n",
        "train_df = train_df.dropna()  # Dropping missing rows for simplicity\n",
        "\n",
        "# Validate no more missing data\n",
        "print(train_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "4jVD9DFl2NmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### feature extraction"
      ],
      "metadata": {
        "id": "RXz0sC6J2Vgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Remove the last fully connected layer to get feature vectors\n",
        "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Image transformations - resizing, normalization (assuming ImageNet normalization)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Example function to extract features\n",
        "def extract_image_features(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    img_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        features = model(img_tensor)\n",
        "    return features.squeeze().numpy()\n",
        "\n",
        "# Example usage\n",
        "image_path = \"downloaded_images/image_0.jpg\"\n",
        "features = extract_image_features(image_path)\n",
        "print(features)\n"
      ],
      "metadata": {
        "id": "qfYp7Lcz2X_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract\n",
        "sudo apt-get install tesseract-ocr\n"
      ],
      "metadata": {
        "id": "9FkYib5W2jzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "# Example function to extract text from image using Tesseract OCR\n",
        "def extract_text_from_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "text = extract_text_from_image(image_path)\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "J8QSclfh2mBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install easyocr\n"
      ],
      "metadata": {
        "id": "frhVM0Pk2oiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr\n",
        "\n",
        "# Initialize EasyOCR reader\n",
        "reader = easyocr.Reader(['en'])\n",
        "\n",
        "# Function to extract text using EasyOCR\n",
        "def extract_text_easyocr(image_path):\n",
        "    results = reader.readtext(image_path)\n",
        "    return \" \".join([res[1] for res in results])\n",
        "\n",
        "# Example usage\n",
        "ocr_text = extract_text_easyocr(image_path)\n",
        "print(ocr_text)\n"
      ],
      "metadata": {
        "id": "YKDPkwVF2qUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "# Define the multi-task learning model\n",
        "class EntityPredictionModel(nn.Module):\n",
        "    def __init__(self, num_units):\n",
        "        super(EntityPredictionModel, self).__init__()\n",
        "\n",
        "        # Pre-trained ResNet model for feature extraction\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()  # Remove the last fully connected layer\n",
        "\n",
        "        # Task 1: Predicting the numeric value (regression)\n",
        "        self.fc_value = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)  # Output a single numeric value\n",
        "        )\n",
        "\n",
        "        # Task 2: Predicting the unit (classification)\n",
        "        self.fc_unit = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_units),  # Output logits for each unit\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the CNN (ResNet) to get features\n",
        "        features = self.resnet(x)\n",
        "\n",
        "        # Predict the numeric value\n",
        "        value = self.fc_value(features)\n",
        "\n",
        "        # Predict the unit\n",
        "        unit = self.fc_unit(features)\n",
        "\n",
        "        return value, unit\n",
        "\n",
        "# Example usage\n",
        "num_units = 10  # Replace with actual number of allowed units from constants.py\n",
        "model = EntityPredictionModel(num_units)\n"
      ],
      "metadata": {
        "id": "f18OtR7t2yoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, num_epochs=25):\n",
        "    criterion_value = nn.MSELoss()  # Loss for predicting the numeric value (regression)\n",
        "    criterion_unit = nn.CrossEntropyLoss()  # Loss for predicting the unit (classification)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels_value, labels_unit in dataloader:\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs_value, outputs_unit = model(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss_value = criterion_value(outputs_value, labels_value)\n",
        "            loss_unit = criterion_unit(outputs_unit, labels_unit)\n",
        "            loss = loss_value + loss_unit\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}')\n",
        "\n",
        "    print('Training complete')\n",
        "\n",
        "# Example: Assuming a DataLoader for the dataset exists\n",
        "# train_model(model, train_dataloader)\n"
      ],
      "metadata": {
        "id": "mWZ2OTXq2zzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prediction(value, unit):\n",
        "    \"\"\"\n",
        "    Format the prediction as '<value> <unit>'.\n",
        "    \"\"\"\n",
        "    formatted_value = f\"{value:.2f}\"  # Format to two decimal places\n",
        "    return f\"{formatted_value} {unit}\"\n",
        "\n",
        "# Example usage\n",
        "predicted_value = 2.567  # Example predicted value\n",
        "predicted_unit = 'gram'  # Example predicted unit\n",
        "formatted_prediction = format_prediction(predicted_value, predicted_unit)\n",
        "print(formatted_prediction)  # Output: \"2.57 gram\"\n"
      ],
      "metadata": {
        "id": "p2DxNrby206n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def make_predictions(model, test_dataloader):\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, index in test_dataloader:\n",
        "            # Get predictions\n",
        "            value_pred, unit_pred = model(inputs)\n",
        "\n",
        "            # Convert logits to predicted unit (get the index of the highest probability)\n",
        "            _, unit_idx = torch.max(unit_pred, 1)\n",
        "\n",
        "            # Format the prediction\n",
        "            prediction = format_prediction(value_pred.item(), unit_idx.item())\n",
        "            predictions.append((index.item(), prediction))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Format the results into a CSV file for submission\n",
        "def save_predictions_to_csv(predictions, output_file=\"submission.csv\"):\n",
        "    df = pd.DataFrame(predictions, columns=[\"index\", \"prediction\"])\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "# Example: Assuming test_dataloader is available\n",
        "# predictions = make_predictions(model, test_dataloader)\n",
        "# save_predictions_to_csv(predictions)\n"
      ],
      "metadata": {
        "id": "U-k-BWXL25_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}